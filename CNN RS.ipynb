{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "powerful-motorcycle",
   "metadata": {},
   "source": [
    "# CNN RS\n",
    "* Model 1: X: tokenized body and y is an array of article ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "material-bundle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "removed-pearl",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"articles.bin\", \"rb\") as f_in:\n",
    "    articles = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "moderate-clause",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"user_bahaviors.bin\", \"rb\") as f_in:\n",
    "    behaviors = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "joined-croatia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>kw_category</th>\n",
       "      <th>article</th>\n",
       "      <th>title_cleaned</th>\n",
       "      <th>category_preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fcc01a7a1a7f7092a2da6b9c5186fdef421c8ab6</td>\n",
       "      <td>pål solberg</td>\n",
       "      <td>Det er Trøndelag politidistrikt som klokken 1...</td>\n",
       "      <td>- Dette er ingen lekeplass</td>\n",
       "      <td>http://www.adressa.no/nyheter/sortrondelag/201...</td>\n",
       "      <td>nyheter sortrondelag</td>\n",
       "      <td>73905</td>\n",
       "      <td>lekeplass</td>\n",
       "      <td>[nyheter, sortrondelag]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 article_id       author  \\\n",
       "0  fcc01a7a1a7f7092a2da6b9c5186fdef421c8ab6  pål solberg   \n",
       "\n",
       "                                                body  \\\n",
       "0   Det er Trøndelag politidistrikt som klokken 1...   \n",
       "\n",
       "                        title  \\\n",
       "0  - Dette er ingen lekeplass   \n",
       "\n",
       "                                                 url            kw_category  \\\n",
       "0  http://www.adressa.no/nyheter/sortrondelag/201...  nyheter sortrondelag    \n",
       "\n",
       "   article title_cleaned    category_preprocessed  \n",
       "0    73905     lekeplass  [nyheter, sortrondelag]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "articles.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "first-backup",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>kw_category</th>\n",
       "      <th>article</th>\n",
       "      <th>title_cleaned</th>\n",
       "      <th>category_preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [article_id, author, body, title, url, kw_category, article, title_cleaned, category_preprocessed]\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[articles[\"article_id\"] == \"e1f0d81ed8ccb738db28fdfaa51ad3a6b3fc2b8e\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "known-linux",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242916\n"
     ]
    }
   ],
   "source": [
    "print(len(behaviors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "british-harassment",
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors = behaviors.iloc[0:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "superb-cathedral",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [03:35, 231.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total error count:  17095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_all_bodies(behaviors, articles):\n",
    "    \"\"\"\n",
    "    return: \n",
    "        list of bodies\n",
    "    \"\"\"\n",
    "    bodies = []\n",
    "    categories = []\n",
    "    error_array = []\n",
    "    error_indices_array = []\n",
    "    error_count = 0\n",
    "    for index, row in tqdm(behaviors.iterrows()):\n",
    "        article_id = row[\"id\"]\n",
    "        try:\n",
    "            article = articles[articles[\"article_id\"] == article_id]\n",
    "            body = article.body.values[0] # body data\n",
    "            cat = article.category_preprocessed.values[0] # categoriy data\n",
    "            if len(body) == 0 or article.empty: # article does not exist\n",
    "                bodies.append(\"Body is empty\")\n",
    "                categories.append(\"Category is empty\")\n",
    "                error_array.append(article_id)\n",
    "                error_indices_array.append(index)\n",
    "                error_count += 1\n",
    "                \n",
    "            else:\n",
    "                bodies.append(body)\n",
    "                categories.append(cat)\n",
    "        except:\n",
    "            error_array.append(article_id)\n",
    "            error_indices_array.append(index)\n",
    "            error_count += 1\n",
    "            bodies.append(\"error\")\n",
    "            categories.append(\"error\")\n",
    "    print(\"Total error count: \", error_count)\n",
    "    return bodies, categories, error_array, error_indices_array\n",
    "bodies,categories, error_array, error_indices_array = get_all_bodies(behaviors, articles)\n",
    "\n",
    "assert behaviors.shape[0] == len(bodies) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "liberal-tours",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eivindfalun/opt/anaconda3/envs/dnnrs/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/eivindfalun/opt/anaconda3/envs/dnnrs/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "behaviors[\"body\"] = bodies\n",
    "behaviors[\"category\"] = categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "specialized-struggle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 10)\n",
      "(32905, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eivindfalun/opt/anaconda3/envs/dnnrs/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/eivindfalun/opt/anaconda3/envs/dnnrs/lib/python3.7/site-packages/pandas/core/frame.py:4312: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "behaviors[\"body\"] = bodies\n",
    "print(behaviors.shape)\n",
    "behaviors.drop(error_indices_array, inplace=True)\n",
    "print(behaviors.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "alert-methodology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>userId</th>\n",
       "      <th>userFreq</th>\n",
       "      <th>articleId</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>id</th>\n",
       "      <th>time</th>\n",
       "      <th>body</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>cx:0d6120e0df4899ed1f18e5377c62644a:liav87wp9vf6</td>\n",
       "      <td>9</td>\n",
       "      <td>4390</td>\n",
       "      <td>Slik blir ferieåret 2017</td>\n",
       "      <td>frank lervik</td>\n",
       "      <td>f2ce698b3daf00cfcac0d5279053c4da9de07a92</td>\n",
       "      <td>1483290441</td>\n",
       "      <td>I Norge har vi ti bevegelige helligdager som ...</td>\n",
       "      <td>[nyheter, sortrondelag]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>cx:0d6120e0df4899ed1f18e5377c62644a:liav87wp9vf6</td>\n",
       "      <td>9</td>\n",
       "      <td>931</td>\n",
       "      <td>Bolig totalskadd i brann</td>\n",
       "      <td>frank lervik</td>\n",
       "      <td>338d849c5c3e0a320d91a2ed2026e43e7c17f8dc</td>\n",
       "      <td>1483260587</td>\n",
       "      <td>Like før klokken to natt til 1 nyttårsdag fik...</td>\n",
       "      <td>[nyheter, moreromsdal]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user                                            userId  userFreq  \\\n",
       "0    13  cx:0d6120e0df4899ed1f18e5377c62644a:liav87wp9vf6         9   \n",
       "1    13  cx:0d6120e0df4899ed1f18e5377c62644a:liav87wp9vf6         9   \n",
       "\n",
       "   articleId                     title        author  \\\n",
       "0       4390  Slik blir ferieåret 2017  frank lervik   \n",
       "1        931  Bolig totalskadd i brann  frank lervik   \n",
       "\n",
       "                                         id        time  \\\n",
       "0  f2ce698b3daf00cfcac0d5279053c4da9de07a92  1483290441   \n",
       "1  338d849c5c3e0a320d91a2ed2026e43e7c17f8dc  1483260587   \n",
       "\n",
       "                                                body                 category  \n",
       "0   I Norge har vi ti bevegelige helligdager som ...  [nyheter, sortrondelag]  \n",
       "1   Like før klokken to natt til 1 nyttårsdag fik...   [nyheter, moreromsdal]  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behaviors.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-tucson",
   "metadata": {},
   "source": [
    "# NLP preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "utility-pontiac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/eivindfalun/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "prescribed-token",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def _removeNonAscii(s):\n",
    "    return \"\".join(i for i in s if  ord(i)<128)\n",
    "\n",
    "def make_lower_case(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    text = text.split()\n",
    "    stops = set(stopwords.words(\"norwegian\"))\n",
    "    text = [w for w in text if not w in stops]\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "def remove_html(text):\n",
    "    html_pattern = re.compile('<.*?>')\n",
    "    return html_pattern.sub(r'', text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "def text_to_list(text):\n",
    "    text = text.split(\" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-kuwait",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "overall-arlington",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article(id, df_articles):\n",
    "    try:\n",
    "        news_article = df_articles[df_articles.article_id == id]\n",
    "        return news_article\n",
    "    except:\n",
    "        print(\"Error: cant find article\")\n",
    "#t = get_article(\"338d849c5c3e0a320d91a2ed2026e43e7c17f8dc\", articles)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-investigator",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "mathematical-fault",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eivindfalun/opt/anaconda3/envs/dnnrs/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Users/eivindfalun/opt/anaconda3/envs/dnnrs/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/eivindfalun/opt/anaconda3/envs/dnnrs/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/eivindfalun/opt/anaconda3/envs/dnnrs/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "/Users/eivindfalun/opt/anaconda3/envs/dnnrs/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Users/eivindfalun/opt/anaconda3/envs/dnnrs/lib/python3.7/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/eivindfalun/opt/anaconda3/envs/dnnrs/lib/python3.7/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "def clean_body(df):\n",
    "    df[\"body_cleaned\"] = df.body.apply(func = make_lower_case)\n",
    "    df[\"body_cleaned\"] = df.body_cleaned.apply(func = remove_stop_words)\n",
    "    df[\"body_cleaned\"] = df.body_cleaned.apply(func = remove_punctuation)\n",
    "    df[\"body_list\"] = df.body_cleaned.apply(func = text_to_list)\n",
    "    return df\n",
    "prep_articles = clean_body(behaviors)\n",
    "\n",
    "def clean_title(df):\n",
    "    df[\"title_cleaned\"] = df.title.apply(func = make_lower_case)\n",
    "    df[\"title_cleaned\"] = df.title_cleaned.apply(func = remove_stop_words)\n",
    "    df[\"title_cleaned\"] = df.title_cleaned.apply(func = remove_punctuation)\n",
    "    return df\n",
    "prep_articles = clean_title(behaviors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "functioning-cancellation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>userId</th>\n",
       "      <th>userFreq</th>\n",
       "      <th>articleId</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>id</th>\n",
       "      <th>time</th>\n",
       "      <th>body</th>\n",
       "      <th>category</th>\n",
       "      <th>body_cleaned</th>\n",
       "      <th>body_list</th>\n",
       "      <th>title_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>cx:0d6120e0df4899ed1f18e5377c62644a:liav87wp9vf6</td>\n",
       "      <td>9</td>\n",
       "      <td>4390</td>\n",
       "      <td>Slik blir ferieåret 2017</td>\n",
       "      <td>frank lervik</td>\n",
       "      <td>f2ce698b3daf00cfcac0d5279053c4da9de07a92</td>\n",
       "      <td>1483290441</td>\n",
       "      <td>I Norge har vi ti bevegelige helligdager som ...</td>\n",
       "      <td>[nyheter, sortrondelag]</td>\n",
       "      <td>norge ti bevegelige helligdager falle hverdage...</td>\n",
       "      <td>[norge, ti, bevegelige, helligdager, falle, hv...</td>\n",
       "      <td>ferieåret 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>cx:0d6120e0df4899ed1f18e5377c62644a:liav87wp9vf6</td>\n",
       "      <td>9</td>\n",
       "      <td>931</td>\n",
       "      <td>Bolig totalskadd i brann</td>\n",
       "      <td>frank lervik</td>\n",
       "      <td>338d849c5c3e0a320d91a2ed2026e43e7c17f8dc</td>\n",
       "      <td>1483260587</td>\n",
       "      <td>Like før klokken to natt til 1 nyttårsdag fik...</td>\n",
       "      <td>[nyheter, moreromsdal]</td>\n",
       "      <td>like klokken to natt 1 nyttårsdag fikk politie...</td>\n",
       "      <td>[like, klokken, to, natt, 1, nyttårsdag, fikk,...</td>\n",
       "      <td>bolig totalskadd brann</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user                                            userId  userFreq  \\\n",
       "0    13  cx:0d6120e0df4899ed1f18e5377c62644a:liav87wp9vf6         9   \n",
       "1    13  cx:0d6120e0df4899ed1f18e5377c62644a:liav87wp9vf6         9   \n",
       "\n",
       "   articleId                     title        author  \\\n",
       "0       4390  Slik blir ferieåret 2017  frank lervik   \n",
       "1        931  Bolig totalskadd i brann  frank lervik   \n",
       "\n",
       "                                         id        time  \\\n",
       "0  f2ce698b3daf00cfcac0d5279053c4da9de07a92  1483290441   \n",
       "1  338d849c5c3e0a320d91a2ed2026e43e7c17f8dc  1483260587   \n",
       "\n",
       "                                                body                 category  \\\n",
       "0   I Norge har vi ti bevegelige helligdager som ...  [nyheter, sortrondelag]   \n",
       "1   Like før klokken to natt til 1 nyttårsdag fik...   [nyheter, moreromsdal]   \n",
       "\n",
       "                                        body_cleaned  \\\n",
       "0  norge ti bevegelige helligdager falle hverdage...   \n",
       "1  like klokken to natt 1 nyttårsdag fikk politie...   \n",
       "\n",
       "                                           body_list           title_cleaned  \n",
       "0  [norge, ti, bevegelige, helligdager, falle, hv...          ferieåret 2017  \n",
       "1  [like, klokken, to, natt, 1, nyttårsdag, fikk,...  bolig totalskadd brann  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_articles.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-handbook",
   "metadata": {},
   "source": [
    "### Tokenize and pad sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-springfield",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "article_enc = LabelEncoder()\n",
    "prep_articles[\"newsId\"] = article_enc.fit_transform(prep_articles[\"articleId\"].values)\n",
    "y = prep_articles[\"newsId\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lovely-armstrong",
   "metadata": {},
   "outputs": [],
   "source": [
    "myset = set(y)\n",
    "temp = len(myset)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-government",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(prep_articles[\"newsId\"].unique())\n",
    "assert num_classes == len(set(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-liberal",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = prep_articles.body_cleaned.values\n",
    "sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "global-garage",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sentences))\n",
    "print(len(sentences_train), len(sentences_test))\n",
    "print(len(y_train), len(y_test))\n",
    "print(sentences_train[0])\n",
    "print(y_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-tenant",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(sentences_train)\n",
    "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
    "X_test = tokenizer.texts_to_sequences(sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-universal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-price",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-punch",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "maxlen = 150\n",
    "X_train = pad_sequences(X_train, padding=\"post\", maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding=\"post\", maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naked-institution",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from gensim.models import Word2Vec\n",
    "w2v_model = Word2Vec(min_count=20, window=2, size=100)\n",
    "w2v_model.build_vocab(prep_articles.body_list.values, progress_per=10000)\n",
    "w2v_model.train(prep_articles.body_list.values, total_examples=w2v_model.corpus_count, epochs=30)\n",
    "vec = w2v_model.wv.vocab.keys()\n",
    "vec\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-reader",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-welsh",
   "metadata": {},
   "source": [
    "# Test 1: without word2vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-impossible",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index)  + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-airport",
   "metadata": {},
   "outputs": [],
   "source": [
    "#article_enc = LabelEncoder()\n",
    "#prep_articles[\"newsId\"] = article_enc.fit_transform(prep_articles[\"articleId\"].values)\n",
    "#y = prep_articles[\"newsId\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floral-destiny",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = prep_articles[\"newsId\"].values\n",
    "#print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "studied-reputation",
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graduate-gambling",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0])\n",
    "print(len(X_train))\n",
    "#print(\"X shape: \", X_train.shape)\n",
    "print(y_train[0])\n",
    "print(len(y_train))\n",
    "assert len(y_train) == len(X_train)\n",
    "assert len(y_test) == len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-fisher",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "august-indiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-light",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-deadline",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-mouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "\n",
    "def build_model():\n",
    "    input_item = keras.Input(shape=(150,))\n",
    "    embedding = layers.Embedding(input_dim=vocab_size,\n",
    "                                output_dim=50,\n",
    "                                input_length=maxlen)(input_item) # input_length is the length of the sequence of words in a sentence. is typically used when having a sequence of symbols as input (think sequence of words).\n",
    "    conv = layers.Conv1D(128, 5, activation=\"relu\")(embedding)\n",
    "    pool = layers.GlobalMaxPooling1D()(conv) # puts it into a lower dimension\n",
    "    \n",
    "    # classification\n",
    "    x = layers.Flatten()(pool)\n",
    "    #x = layers.Dense(3000, activation=\"relu\")(x)\n",
    "    x = layers.Dense(num_classes*10, activation=\"relu\")(x) # new\n",
    "    x = layers.Dense(num_classes*5, activation=\"relu\")(x) # new\n",
    "    output = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    \n",
    "    model = keras.Model(input_item, output)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss = tf.losses.sparse_categorical_crossentropy,\n",
    "        metrics=[\"accuracy\", \"sparse_categorical_accuracy\"]\n",
    "        \n",
    "    )\n",
    "    return model\n",
    "# Note to self; sparse-categorical-crossentropy requires target value to be one integers. \n",
    "#We can do the exact same (and get exact same results) when one-hot encoding y and use categorical_crossentropy\n",
    "\"\"\"\n",
    "This is a quote: \"Loss function- Here I have used sparse_categorical_crossentropy since we are trying \n",
    "to reduce the loss between the next predicted video and what the user has actually \n",
    "watched from the video corpus. In order to deal with highly frequent ids from polluting \n",
    "the predictions (for example, causing the network to suggest the most frequently viewed videos to all users), \n",
    "we can use the concept of negative sampling with the help of “sampled softmax loss” as implemented in TensorFlow. \n",
    "But we will stick to the simple softmax layer in our example.\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-michigan",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invalid-integration",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, \n",
    "                    y_train, \n",
    "                    epochs=10, \n",
    "                    batch_size=64,\n",
    "                    validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "private-response",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy,_ = model.evaluate(X_train, y_train)\n",
    "print(accuracy)\n",
    "loss, accuracy,_ = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-consequence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def plot_history(history):\n",
    "    # Source: https://realpython.com/python-keras-text-classification/\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unnecessary-carrier",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-regulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-certification",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xnew = X_train[1]\n",
    "Xnew.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-scheme",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, X):\n",
    "    # predicitons\n",
    "    preds = model.predict(X_test)\n",
    "    #fix predictions\n",
    "    top_one = []\n",
    "    top_five = []\n",
    "    for elem in preds:\n",
    "        top = np.argmax(elem)\n",
    "        top_one.append(top)\n",
    "        indices = np.argsort(elem)[:-6:-1]\n",
    "        top_five.append(indices)\n",
    "    return preds, top_one, np.array(top_five)\n",
    "\n",
    "# predicitons\n",
    "preds, top_one, top_five = get_predictions(model,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greenhouse-force",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detected-reflection",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-heater",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preds, y):\n",
    "    assert len(preds) == len(y)\n",
    "    acc = 0\n",
    "    for i in range(len(preds)):\n",
    "        if preds[i] == y[i]:\n",
    "            acc += 1\n",
    "    return acc / len(preds)\n",
    "a = accuracy(top_one, y_test)\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-democracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_k(y_predictions, y):\n",
    "    n, k = y_predictions.shape\n",
    "    tp = 0 #true positive\n",
    "    for i in tqdm(range(n)):\n",
    "        for j in range(k):\n",
    "            if y_predictions[i,j] == y[i]:\n",
    "                tp += 1\n",
    "    precision = tp / (n*k)\n",
    "    print(\"Precision @\",k, \":\",precision)\n",
    "    return precision\n",
    "p = precision_k(np.array(top_five), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discrete-definition",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = np.array(top_five)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-newport",
   "metadata": {},
   "outputs": [],
   "source": [
    "k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-blink",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommend five most clicked articles\n",
    "top = prep_articles.newsId.value_counts().head(5).values\n",
    "num_groups = len(y_train)\n",
    "baseline = np.tile(top, num_groups).reshape(-1,5)\n",
    "baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-sullivan",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = precision_k(baseline, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polyphonic-cleaners",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test # contains newsId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-criticism",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test #tokenized body of the news article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-slope",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode y\n",
    "def build_model2():\n",
    "    input_item = keras.Input(shape=(150,))\n",
    "    embedding = layers.Embedding(input_dim=vocab_size,\n",
    "                                output_dim=50,\n",
    "                                input_length=maxlen)(input_item)\n",
    "    conv = layers.Conv1D(128, 5, activation=\"relu\")(embedding)\n",
    "    pool = layers.GlobalMaxPooling1D()(conv) # puts it into a lower dimension\n",
    "    \n",
    "    # classification\n",
    "    x = layers.Flatten()(pool)\n",
    "    x = layers.Dense(3000, activation=\"relu\")(x)\n",
    "    output = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    \n",
    "    model = keras.Model(input_item, output)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss = tf.losses.categorical_crossentropy,\n",
    "        metrics=[\"accuracy\", \"categorical_accuracy\"]\n",
    "        \n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-symbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "model2 = build_model2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-floating",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new = to_categorical(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incomplete-glenn",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-picture",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_new.shape)\n",
    "print(X_train.shape)\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-variable",
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = model2.fit(X_train, \n",
    "                    y_new, \n",
    "                    epochs=10, \n",
    "                    batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "color-trace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, X):\n",
    "    # predicitons\n",
    "    preds = model.predict(X_test)\n",
    "    #fix predictions\n",
    "    top_one = []\n",
    "    top_five = []\n",
    "    for elem in preds:\n",
    "        top = np.argmax(elem)\n",
    "        top_one.append(top)\n",
    "        indices = np.argsort(elem)[:-6:-1]\n",
    "        top_five.append(indices)\n",
    "    return preds, top_one, np.array(top_five)\n",
    "\n",
    "# predicitons\n",
    "preds, top_one, top_five = get_predictions(model,X_test)\n",
    "def precision_k(y_predictions, y):\n",
    "    n, k = y_predictions.shape\n",
    "    tp = 0 #true positive\n",
    "    for i in tqdm(range(n)):\n",
    "        for j in range(k):\n",
    "            if y_predictions[i,j] == y[i]:\n",
    "                tp += 1\n",
    "    precision = tp / (n*k)\n",
    "    print(\"Precision @\",k, \":\",precision)\n",
    "    return precision\n",
    "p = precision_k(np.array(top_five), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-process",
   "metadata": {},
   "source": [
    "# Model 2: Train embeddings and then use CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "killing-sitting",
   "metadata": {},
   "source": [
    "# Model 3: CF softmax model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ranking-equity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "hourly-aaron",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>userId</th>\n",
       "      <th>userFreq</th>\n",
       "      <th>articleId</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>id</th>\n",
       "      <th>time</th>\n",
       "      <th>body</th>\n",
       "      <th>category</th>\n",
       "      <th>body_cleaned</th>\n",
       "      <th>body_list</th>\n",
       "      <th>title_cleaned</th>\n",
       "      <th>news_id</th>\n",
       "      <th>author_num</th>\n",
       "      <th>bodies_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7255</td>\n",
       "      <td>cx:0d6120e0df4899ed1f18e5377c62644a:liav87wp9vf6</td>\n",
       "      <td>9</td>\n",
       "      <td>4390</td>\n",
       "      <td>Slik blir ferieåret 2017</td>\n",
       "      <td>frank lervik</td>\n",
       "      <td>f2ce698b3daf00cfcac0d5279053c4da9de07a92</td>\n",
       "      <td>2017-01-01 17:07:21</td>\n",
       "      <td>I Norge har vi ti bevegelige helligdager som ...</td>\n",
       "      <td>[nyheter, sortrondelag]</td>\n",
       "      <td>norge ti bevegelige helligdager falle hverdage...</td>\n",
       "      <td>[norge, ti, bevegelige, helligdager, falle, hv...</td>\n",
       "      <td>ferieåret 2017</td>\n",
       "      <td>457</td>\n",
       "      <td>1</td>\n",
       "      <td>[6, 106, 1061, 1076, 1033, 1054, 205, 2, 86, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7255</td>\n",
       "      <td>cx:0d6120e0df4899ed1f18e5377c62644a:liav87wp9vf6</td>\n",
       "      <td>9</td>\n",
       "      <td>931</td>\n",
       "      <td>Bolig totalskadd i brann</td>\n",
       "      <td>frank lervik</td>\n",
       "      <td>338d849c5c3e0a320d91a2ed2026e43e7c17f8dc</td>\n",
       "      <td>2017-01-01 08:49:47</td>\n",
       "      <td>Like før klokken to natt til 1 nyttårsdag fik...</td>\n",
       "      <td>[nyheter, moreromsdal]</td>\n",
       "      <td>like klokken to natt 1 nyttårsdag fikk politie...</td>\n",
       "      <td>[like, klokken, to, natt, 1, nyttårsdag, fikk,...</td>\n",
       "      <td>bolig totalskadd brann</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>[200, 603, 44, 237, 4, 77, 57, 46, 1416, 3006,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user                                            userId  userFreq  \\\n",
       "0  7255  cx:0d6120e0df4899ed1f18e5377c62644a:liav87wp9vf6         9   \n",
       "1  7255  cx:0d6120e0df4899ed1f18e5377c62644a:liav87wp9vf6         9   \n",
       "\n",
       "   articleId                     title        author  \\\n",
       "0       4390  Slik blir ferieåret 2017  frank lervik   \n",
       "1        931  Bolig totalskadd i brann  frank lervik   \n",
       "\n",
       "                                         id                time  \\\n",
       "0  f2ce698b3daf00cfcac0d5279053c4da9de07a92 2017-01-01 17:07:21   \n",
       "1  338d849c5c3e0a320d91a2ed2026e43e7c17f8dc 2017-01-01 08:49:47   \n",
       "\n",
       "                                                body                 category  \\\n",
       "0   I Norge har vi ti bevegelige helligdager som ...  [nyheter, sortrondelag]   \n",
       "1   Like før klokken to natt til 1 nyttårsdag fik...   [nyheter, moreromsdal]   \n",
       "\n",
       "                                        body_cleaned  \\\n",
       "0  norge ti bevegelige helligdager falle hverdage...   \n",
       "1  like klokken to natt 1 nyttårsdag fikk politie...   \n",
       "\n",
       "                                           body_list           title_cleaned  \\\n",
       "0  [norge, ti, bevegelige, helligdager, falle, hv...          ferieåret 2017   \n",
       "1  [like, klokken, to, natt, 1, nyttårsdag, fikk,...  bolig totalskadd brann   \n",
       "\n",
       "   news_id  author_num                                   bodies_tokenized  \n",
       "0      457           1  [6, 106, 1061, 1076, 1033, 1054, 205, 2, 86, 1...  \n",
       "1      100           1  [200, 603, 44, 237, 4, 77, 57, 46, 1416, 3006,...  "
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_articles[\"time\"] = pd.to_datetime(prep_articles[\"time\"], unit=\"s\")\n",
    "prep_articles[\"author\"].fillna(\"null\", inplace=True)\n",
    "\n",
    "prep_articles.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "static-techno",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_bodies(df):\n",
    "    bodies = df.body_cleaned.values\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(bodies)\n",
    "    bodies_to_num = tokenizer.texts_to_sequences(bodies)\n",
    "    maxlen = 300\n",
    "    #bodies_to_num = pad_sequences(bodies_to_num, padding=\"post\", maxlen=maxlen\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    return bodies_to_num, vocab_size\n",
    "t, vocab_size_body = tokenize_bodies(prep_articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "indirect-somerset",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27045"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "working-david",
   "metadata": {},
   "outputs": [],
   "source": [
    "#t = np.array(t)\n",
    "#t.shape\n",
    "prep_articles[\"bodies_tokenized\"] = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "color-interface",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "medium-steam",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>userId</th>\n",
       "      <th>userFreq</th>\n",
       "      <th>articleId</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>id</th>\n",
       "      <th>time</th>\n",
       "      <th>body</th>\n",
       "      <th>category</th>\n",
       "      <th>body_cleaned</th>\n",
       "      <th>body_list</th>\n",
       "      <th>title_cleaned</th>\n",
       "      <th>news_id</th>\n",
       "      <th>author_num</th>\n",
       "      <th>bodies_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6952</td>\n",
       "      <td>cx:0d6120e0df4899ed1f18e5377c62644a:liav87wp9vf6</td>\n",
       "      <td>9</td>\n",
       "      <td>4390</td>\n",
       "      <td>Slik blir ferieåret 2017</td>\n",
       "      <td>frank lervik</td>\n",
       "      <td>f2ce698b3daf00cfcac0d5279053c4da9de07a92</td>\n",
       "      <td>2017-01-01 17:07:21</td>\n",
       "      <td>I Norge har vi ti bevegelige helligdager som ...</td>\n",
       "      <td>[nyheter, sortrondelag]</td>\n",
       "      <td>norge ti bevegelige helligdager falle hverdage...</td>\n",
       "      <td>[norge, ti, bevegelige, helligdager, falle, hv...</td>\n",
       "      <td>ferieåret 2017</td>\n",
       "      <td>457</td>\n",
       "      <td>1</td>\n",
       "      <td>[6, 106, 1061, 1076, 1033, 1054, 205, 2, 86, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6952</td>\n",
       "      <td>cx:0d6120e0df4899ed1f18e5377c62644a:liav87wp9vf6</td>\n",
       "      <td>9</td>\n",
       "      <td>931</td>\n",
       "      <td>Bolig totalskadd i brann</td>\n",
       "      <td>frank lervik</td>\n",
       "      <td>338d849c5c3e0a320d91a2ed2026e43e7c17f8dc</td>\n",
       "      <td>2017-01-01 08:49:47</td>\n",
       "      <td>Like før klokken to natt til 1 nyttårsdag fik...</td>\n",
       "      <td>[nyheter, moreromsdal]</td>\n",
       "      <td>like klokken to natt 1 nyttårsdag fikk politie...</td>\n",
       "      <td>[like, klokken, to, natt, 1, nyttårsdag, fikk,...</td>\n",
       "      <td>bolig totalskadd brann</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>[200, 603, 44, 237, 4, 77, 57, 46, 1416, 3006,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user                                            userId  userFreq  \\\n",
       "0  6952  cx:0d6120e0df4899ed1f18e5377c62644a:liav87wp9vf6         9   \n",
       "1  6952  cx:0d6120e0df4899ed1f18e5377c62644a:liav87wp9vf6         9   \n",
       "\n",
       "   articleId                     title        author  \\\n",
       "0       4390  Slik blir ferieåret 2017  frank lervik   \n",
       "1        931  Bolig totalskadd i brann  frank lervik   \n",
       "\n",
       "                                         id                time  \\\n",
       "0  f2ce698b3daf00cfcac0d5279053c4da9de07a92 2017-01-01 17:07:21   \n",
       "1  338d849c5c3e0a320d91a2ed2026e43e7c17f8dc 2017-01-01 08:49:47   \n",
       "\n",
       "                                                body                 category  \\\n",
       "0   I Norge har vi ti bevegelige helligdager som ...  [nyheter, sortrondelag]   \n",
       "1   Like før klokken to natt til 1 nyttårsdag fik...   [nyheter, moreromsdal]   \n",
       "\n",
       "                                        body_cleaned  \\\n",
       "0  norge ti bevegelige helligdager falle hverdage...   \n",
       "1  like klokken to natt 1 nyttårsdag fikk politie...   \n",
       "\n",
       "                                           body_list           title_cleaned  \\\n",
       "0  [norge, ti, bevegelige, helligdager, falle, hv...          ferieåret 2017   \n",
       "1  [like, klokken, to, natt, 1, nyttårsdag, fikk,...  bolig totalskadd brann   \n",
       "\n",
       "   news_id  author_num                                   bodies_tokenized  \n",
       "0      457           1  [6, 106, 1061, 1076, 1033, 1054, 205, 2, 86, 1...  \n",
       "1      100           1  [200, 603, 44, 237, 4, 77, 57, 46, 1416, 3006,...  "
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def label_encode(df):\n",
    "    \"\"\"\n",
    "        Labelencode user(id) and news_id\n",
    "    \"\"\"\n",
    "    user_enc = LabelEncoder()\n",
    "    article_enc = LabelEncoder()\n",
    "    \n",
    "    df[\"user\"] = user_enc.fit_transform(df.user.astype(\"str\"))\n",
    "    df.user = df.user.astype(\"int32\")\n",
    "    \n",
    "    df[\"news_id\"] = article_enc.fit_transform(df.id.astype(\"str\"))\n",
    "    df.news_id = df.news_id.astype(\"int32\")\n",
    "    return df\n",
    "temp = label_encode(prep_articles)\n",
    "temp.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "broken-minnesota",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "490"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_articles = len(temp[\"news_id\"].unique())\n",
    "num_users = len(temp[\"user\"].unique())\n",
    "num_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "headed-brake",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "mathematical-tuner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num authors 161\n",
      "161\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>userId</th>\n",
       "      <th>userFreq</th>\n",
       "      <th>articleId</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>id</th>\n",
       "      <th>time</th>\n",
       "      <th>body</th>\n",
       "      <th>category</th>\n",
       "      <th>body_cleaned</th>\n",
       "      <th>body_list</th>\n",
       "      <th>title_cleaned</th>\n",
       "      <th>news_id</th>\n",
       "      <th>author_num</th>\n",
       "      <th>bodies_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6952</td>\n",
       "      <td>cx:0d6120e0df4899ed1f18e5377c62644a:liav87wp9vf6</td>\n",
       "      <td>9</td>\n",
       "      <td>4390</td>\n",
       "      <td>Slik blir ferieåret 2017</td>\n",
       "      <td>frank lervik</td>\n",
       "      <td>f2ce698b3daf00cfcac0d5279053c4da9de07a92</td>\n",
       "      <td>2017-01-01 17:07:21</td>\n",
       "      <td>I Norge har vi ti bevegelige helligdager som ...</td>\n",
       "      <td>[nyheter, sortrondelag]</td>\n",
       "      <td>norge ti bevegelige helligdager falle hverdage...</td>\n",
       "      <td>[norge, ti, bevegelige, helligdager, falle, hv...</td>\n",
       "      <td>ferieåret 2017</td>\n",
       "      <td>457</td>\n",
       "      <td>1</td>\n",
       "      <td>[6, 106, 1061, 1076, 1033, 1054, 205, 2, 86, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6952</td>\n",
       "      <td>cx:0d6120e0df4899ed1f18e5377c62644a:liav87wp9vf6</td>\n",
       "      <td>9</td>\n",
       "      <td>931</td>\n",
       "      <td>Bolig totalskadd i brann</td>\n",
       "      <td>frank lervik</td>\n",
       "      <td>338d849c5c3e0a320d91a2ed2026e43e7c17f8dc</td>\n",
       "      <td>2017-01-01 08:49:47</td>\n",
       "      <td>Like før klokken to natt til 1 nyttårsdag fik...</td>\n",
       "      <td>[nyheter, moreromsdal]</td>\n",
       "      <td>like klokken to natt 1 nyttårsdag fikk politie...</td>\n",
       "      <td>[like, klokken, to, natt, 1, nyttårsdag, fikk,...</td>\n",
       "      <td>bolig totalskadd brann</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>[200, 603, 44, 237, 4, 77, 57, 46, 1416, 3006,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user                                            userId  userFreq  \\\n",
       "0  6952  cx:0d6120e0df4899ed1f18e5377c62644a:liav87wp9vf6         9   \n",
       "1  6952  cx:0d6120e0df4899ed1f18e5377c62644a:liav87wp9vf6         9   \n",
       "\n",
       "   articleId                     title        author  \\\n",
       "0       4390  Slik blir ferieåret 2017  frank lervik   \n",
       "1        931  Bolig totalskadd i brann  frank lervik   \n",
       "\n",
       "                                         id                time  \\\n",
       "0  f2ce698b3daf00cfcac0d5279053c4da9de07a92 2017-01-01 17:07:21   \n",
       "1  338d849c5c3e0a320d91a2ed2026e43e7c17f8dc 2017-01-01 08:49:47   \n",
       "\n",
       "                                                body                 category  \\\n",
       "0   I Norge har vi ti bevegelige helligdager som ...  [nyheter, sortrondelag]   \n",
       "1   Like før klokken to natt til 1 nyttårsdag fik...   [nyheter, moreromsdal]   \n",
       "\n",
       "                                        body_cleaned  \\\n",
       "0  norge ti bevegelige helligdager falle hverdage...   \n",
       "1  like klokken to natt 1 nyttårsdag fikk politie...   \n",
       "\n",
       "                                           body_list           title_cleaned  \\\n",
       "0  [norge, ti, bevegelige, helligdager, falle, hv...          ferieåret 2017   \n",
       "1  [like, klokken, to, natt, 1, nyttårsdag, fikk,...  bolig totalskadd brann   \n",
       "\n",
       "   news_id  author_num                                   bodies_tokenized  \n",
       "0      457           1  [6, 106, 1061, 1076, 1033, 1054, 205, 2, 86, 1...  \n",
       "1      100           1  [200, 603, 44, 237, 4, 77, 57, 46, 1416, 3006,...  "
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gen_authors_unique(df):\n",
    "    res = []\n",
    "    authors = list((df[\"author\"].values))\n",
    "    for i in range(len(authors)):\n",
    "        if isinstance(authors[i], list):\n",
    "            for elem in authors[i]:\n",
    "                if elem not in res:\n",
    "                    res.append(elem)\n",
    "        elif authors[i] not in res:\n",
    "            res.append(authors[i])\n",
    "    print(\"num authors\", len(res))\n",
    "    return res\n",
    "    \n",
    "\n",
    "def generate_author_to_id(author_list):\n",
    "    author_to_id = {name: idx + 1 for idx, name in enumerate(author_list)} # preserve 0 for padding\n",
    "    id_to_author = {idx: name for idx, name in author_to_id.items()}\n",
    "    return author_to_id, id_to_author\n",
    "author_to_id, id_to_author = generate_author_to_id(gen_authors_unique(temp))\n",
    "print(len(author_to_id))\n",
    "#print(author_to_id)\n",
    "#print(id_to_author)\n",
    "def get_author_to_num(df,author_to_id ):\n",
    "    author_res = []\n",
    "    for t in range(len(temp)):\n",
    "        try:\n",
    "            if isinstance(temp.author.iloc[t], list):\n",
    "                _list = temp.author.iloc[t]\n",
    "                idx = author_to_id[_list[0]]\n",
    "            else:\n",
    "                idx = author_to_id[temp[\"author\"].iloc[t]]\n",
    "            author_res.append(idx)\n",
    "        except:\n",
    "            print(\"ERROR:\")\n",
    "            print(temp[\"author\"].iloc[t], type(temp[\"author\"].iloc[t]), isinstance(temp[\"author\"].iloc[t], str))\n",
    "    return author_res\n",
    "temp[\"author_num\"] = get_author_to_num(temp, author_to_id)\n",
    "\n",
    "temp.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "given-burden",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "geological-diabetes",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>author</th>\n",
       "      <th>bodies_tokenized</th>\n",
       "      <th>label</th>\n",
       "      <th>news_id_0</th>\n",
       "      <th>news_id_1</th>\n",
       "      <th>news_id_2</th>\n",
       "      <th>news_id_3</th>\n",
       "      <th>news_id_4</th>\n",
       "      <th>news_id_5</th>\n",
       "      <th>...</th>\n",
       "      <th>author_num_6</th>\n",
       "      <th>author_num_7</th>\n",
       "      <th>bodies_tokenized_0</th>\n",
       "      <th>bodies_tokenized_1</th>\n",
       "      <th>bodies_tokenized_2</th>\n",
       "      <th>bodies_tokenized_3</th>\n",
       "      <th>bodies_tokenized_4</th>\n",
       "      <th>bodies_tokenized_5</th>\n",
       "      <th>bodies_tokenized_6</th>\n",
       "      <th>bodies_tokenized_7</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[joakim slettebak wangen, frank lervik, harry ...</td>\n",
       "      <td>[[54, 42, 527, 181, 65, 124, 85, 529, 188, 13,...</td>\n",
       "      <td>17</td>\n",
       "      <td>331</td>\n",
       "      <td>457</td>\n",
       "      <td>209</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[54, 42, 527, 181, 65, 124, 85, 529, 188, 13, ...</td>\n",
       "      <td>[6, 106, 1061, 1076, 1033, 1054, 205, 2, 86, 1...</td>\n",
       "      <td>[54, 132, 333, 343, 42, 26, 46, 344, 336, 88, ...</td>\n",
       "      <td>[51, 300, 778, 940, 94, 849, 272, 959, 960, 8,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[null, birger løfaldli, harry tiller, birger l...</td>\n",
       "      <td>[[603, 1696, 144, 57, 1416, 1697, 1700, 1449, ...</td>\n",
       "      <td>185</td>\n",
       "      <td>148</td>\n",
       "      <td>189</td>\n",
       "      <td>209</td>\n",
       "      <td>3</td>\n",
       "      <td>307</td>\n",
       "      <td>209</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>[603, 1696, 144, 57, 1416, 1697, 1700, 1449, 1...</td>\n",
       "      <td>[68, 74, 618, 634, 587, 158, 37, 105, 635, 241...</td>\n",
       "      <td>[54, 132, 333, 343, 42, 26, 46, 344, 336, 88, ...</td>\n",
       "      <td>[68, 74, 444, 2, 35, 5907, 5908, 44, 260, 5342...</td>\n",
       "      <td>[46, 1317, 1940, 603, 1421, 768, 237, 65, 2040...</td>\n",
       "      <td>[54, 132, 333, 343, 42, 26, 46, 344, 336, 88, ...</td>\n",
       "      <td>[51, 300, 778, 940, 94, 849, 272, 959, 960, 8,...</td>\n",
       "      <td>[182, 2199, 259, 13, 185, 1, 90, 1508, 2197, 2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user                                             author  \\\n",
       "user                                                            \n",
       "0        0  [joakim slettebak wangen, frank lervik, harry ...   \n",
       "1        1  [null, birger løfaldli, harry tiller, birger l...   \n",
       "\n",
       "                                       bodies_tokenized  label  news_id_0  \\\n",
       "user                                                                        \n",
       "0     [[54, 42, 527, 181, 65, 124, 85, 529, 188, 13,...     17        331   \n",
       "1     [[603, 1696, 144, 57, 1416, 1697, 1700, 1449, ...    185        148   \n",
       "\n",
       "      news_id_1  news_id_2  news_id_3  news_id_4  news_id_5  ...  \\\n",
       "user                                                         ...   \n",
       "0           457        209          0          0          0  ...   \n",
       "1           189        209          3        307        209  ...   \n",
       "\n",
       "      author_num_6  author_num_7  \\\n",
       "user                               \n",
       "0                0             0   \n",
       "1               13             7   \n",
       "\n",
       "                                     bodies_tokenized_0  \\\n",
       "user                                                      \n",
       "0     [54, 42, 527, 181, 65, 124, 85, 529, 188, 13, ...   \n",
       "1     [603, 1696, 144, 57, 1416, 1697, 1700, 1449, 1...   \n",
       "\n",
       "                                     bodies_tokenized_1  \\\n",
       "user                                                      \n",
       "0     [6, 106, 1061, 1076, 1033, 1054, 205, 2, 86, 1...   \n",
       "1     [68, 74, 618, 634, 587, 158, 37, 105, 635, 241...   \n",
       "\n",
       "                                     bodies_tokenized_2  \\\n",
       "user                                                      \n",
       "0     [54, 132, 333, 343, 42, 26, 46, 344, 336, 88, ...   \n",
       "1     [54, 132, 333, 343, 42, 26, 46, 344, 336, 88, ...   \n",
       "\n",
       "                                     bodies_tokenized_3  \\\n",
       "user                                                      \n",
       "0     [51, 300, 778, 940, 94, 849, 272, 959, 960, 8,...   \n",
       "1     [68, 74, 444, 2, 35, 5907, 5908, 44, 260, 5342...   \n",
       "\n",
       "                                     bodies_tokenized_4  \\\n",
       "user                                                      \n",
       "0                                                     0   \n",
       "1     [46, 1317, 1940, 603, 1421, 768, 237, 65, 2040...   \n",
       "\n",
       "                                     bodies_tokenized_5  \\\n",
       "user                                                      \n",
       "0                                                     0   \n",
       "1     [54, 132, 333, 343, 42, 26, 46, 344, 336, 88, ...   \n",
       "\n",
       "                                     bodies_tokenized_6  \\\n",
       "user                                                      \n",
       "0                                                     0   \n",
       "1     [51, 300, 778, 940, 94, 849, 272, 959, 960, 8,...   \n",
       "\n",
       "                                     bodies_tokenized_7  \n",
       "user                                                     \n",
       "0                                                     0  \n",
       "1     [182, 2199, 259, 13, 185, 1, 90, 1508, 2197, 2...  \n",
       "\n",
       "[2 rows x 28 columns]"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pack_items(df):\n",
    "    # dont need this\n",
    "    result = []\n",
    "    for user in df.user.unique():\n",
    "        result.append(df.newsId[df.user == user].values)\n",
    "    return result\n",
    "#news_history = pack_items(prep_articles)\n",
    "\n",
    "def group_by_user(df):\n",
    "    grouped = df.sort_values(by=\"time\").groupby(\"user\").agg(\n",
    "        {\"user\": \"mean\", \n",
    "         \"news_id\": lambda mId: list(mId),\n",
    "         \"author\": lambda author: list(author),\n",
    "         \"author_num\": lambda author_num: list(author_num),\n",
    "         \"bodies_tokenized\": lambda bodies_tok: list(bodies_tok)\n",
    "        })\n",
    "    return grouped\n",
    "t = group_by_user(temp)\n",
    "#print(t.head(2))\n",
    "\n",
    "def get_token_count(df):\n",
    "    pass\n",
    "\n",
    "def onehotencode_news(df, column, max_len, padding):\n",
    "    i = 0\n",
    "    for i in range(max_len):\n",
    "        df[column + \"_\" + str(i)] = df[column].apply(lambda x: x[i] if i < len(x) else padding)\n",
    "    df.drop(column, axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "def onehotencode_author(df, column, max_len, padding):\n",
    "    i = 0\n",
    "    for i in range(max_len):\n",
    "        df[column + \"_\" + str(i)] = df[column].apply(lambda x: x[i] if i < len(x) else padding)\n",
    "    df.drop(column, axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "def split_bodies(df, column, max_len, padding):\n",
    "    # not working\n",
    "    i = 0\n",
    "    for i in range(max_len):\n",
    "        df[column + \"_\" + str(i)] = df[column].apply(lambda x: x[i] if i < len(x) else padding)\n",
    "    return df\n",
    "\n",
    "def onehotencode_bodies(df, column, max_len, padding):\n",
    "    i = 0\n",
    "    for i in range(max_len):\n",
    "        df[column + \"_\" + str(i)] = df[column].apply(lambda x: x[i] if i < len(x) else padding)\n",
    "    return df\n",
    "    \n",
    "        \n",
    "\n",
    "def prepare_train_test(df, max_hist_len, max_author_len):\n",
    "    #TODO: remove label\n",
    "    df[\"label\"] = df[\"news_id\"].apply(lambda x: x[-1])\n",
    "    df[\"news_id\"] = df[\"news_id\"].apply(lambda x: x[:-1])\n",
    "    \n",
    "    df = onehotencode_news(df, \"news_id\", max_hist_len, padding=0)\n",
    "    df = onehotencode_author(df, \"author_num\",max_author_len, padding=0 )\n",
    "    df = split_bodies(df, \"bodies_tokenized\", 8, padding=0)\n",
    "    return df\n",
    "t = prepare_train_test(t,max_hist_len=8, max_author_len=8 )\n",
    "t.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-pierce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "adopted-stretch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[54,\n",
       " 42,\n",
       " 527,\n",
       " 181,\n",
       " 65,\n",
       " 124,\n",
       " 85,\n",
       " 529,\n",
       " 188,\n",
       " 13,\n",
       " 48,\n",
       " 400,\n",
       " 146,\n",
       " 157,\n",
       " 197,\n",
       " 531,\n",
       " 436,\n",
       " 313,\n",
       " 528,\n",
       " 514,\n",
       " 532,\n",
       " 524,\n",
       " 144,\n",
       " 118,\n",
       " 65,\n",
       " 124]"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.bodies_tokenized_0.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "charitable-destruction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[68,\n",
       " 74,\n",
       " 618,\n",
       " 634,\n",
       " 587,\n",
       " 158,\n",
       " 37,\n",
       " 105,\n",
       " 635,\n",
       " 241,\n",
       " 574,\n",
       " 601,\n",
       " 619,\n",
       " 636,\n",
       " 231,\n",
       " 595,\n",
       " 302,\n",
       " 627,\n",
       " 4,\n",
       " 119,\n",
       " 640,\n",
       " 45,\n",
       " 43,\n",
       " 249,\n",
       " 250,\n",
       " 590,\n",
       " 287,\n",
       " 108,\n",
       " 110,\n",
       " 109,\n",
       " 6,\n",
       " 187,\n",
       " 69,\n",
       " 72,\n",
       " 41,\n",
       " 6,\n",
       " 128,\n",
       " 613,\n",
       " 641,\n",
       " 584,\n",
       " 263,\n",
       " 620,\n",
       " 642,\n",
       " 617,\n",
       " 178,\n",
       " 637,\n",
       " 643,\n",
       " 45,\n",
       " 101,\n",
       " 423,\n",
       " 628,\n",
       " 644,\n",
       " 216,\n",
       " 82,\n",
       " 22,\n",
       " 6,\n",
       " 125,\n",
       " 629,\n",
       " 645,\n",
       " 623,\n",
       " 593,\n",
       " 35,\n",
       " 207,\n",
       " 158,\n",
       " 37,\n",
       " 646,\n",
       " 519,\n",
       " 243,\n",
       " 239,\n",
       " 408,\n",
       " 240,\n",
       " 242,\n",
       " 244,\n",
       " 301,\n",
       " 245,\n",
       " 251,\n",
       " 502,\n",
       " 247,\n",
       " 248,\n",
       " 194,\n",
       " 440,\n",
       " 405,\n",
       " 393,\n",
       " 462,\n",
       " 81,\n",
       " 35,\n",
       " 647,\n",
       " 190,\n",
       " 316,\n",
       " 630,\n",
       " 49,\n",
       " 648,\n",
       " 127,\n",
       " 649,\n",
       " 624,\n",
       " 631,\n",
       " 505,\n",
       " 10,\n",
       " 84,\n",
       " 650,\n",
       " 12,\n",
       " 49,\n",
       " 599,\n",
       " 651,\n",
       " 652,\n",
       " 1,\n",
       " 21,\n",
       " 562,\n",
       " 241,\n",
       " 653,\n",
       " 95,\n",
       " 654,\n",
       " 547,\n",
       " 397,\n",
       " 12,\n",
       " 533,\n",
       " 116,\n",
       " 285,\n",
       " 291,\n",
       " 638,\n",
       " 7,\n",
       " 632,\n",
       " 600,\n",
       " 73,\n",
       " 193,\n",
       " 655,\n",
       " 571,\n",
       " 37,\n",
       " 159,\n",
       " 66,\n",
       " 28,\n",
       " 29,\n",
       " 563,\n",
       " 260,\n",
       " 119,\n",
       " 39,\n",
       " 608,\n",
       " 194,\n",
       " 78,\n",
       " 14,\n",
       " 509,\n",
       " 28,\n",
       " 29,\n",
       " 656,\n",
       " 594,\n",
       " 68,\n",
       " 74,\n",
       " 159,\n",
       " 597,\n",
       " 249,\n",
       " 250,\n",
       " 44,\n",
       " 657,\n",
       " 108,\n",
       " 110,\n",
       " 109,\n",
       " 69,\n",
       " 72,\n",
       " 41,\n",
       " 82,\n",
       " 22,\n",
       " 504,\n",
       " 73,\n",
       " 37,\n",
       " 625,\n",
       " 180,\n",
       " 207,\n",
       " 104,\n",
       " 604,\n",
       " 243,\n",
       " 239,\n",
       " 240,\n",
       " 242,\n",
       " 244,\n",
       " 245,\n",
       " 251,\n",
       " 247,\n",
       " 248,\n",
       " 538,\n",
       " 658,\n",
       " 390,\n",
       " 37,\n",
       " 1,\n",
       " 113,\n",
       " 614,\n",
       " 231,\n",
       " 165,\n",
       " 81,\n",
       " 659,\n",
       " 57,\n",
       " 555,\n",
       " 621,\n",
       " 560,\n",
       " 58,\n",
       " 23,\n",
       " 633,\n",
       " 385,\n",
       " 40,\n",
       " 293,\n",
       " 615,\n",
       " 607,\n",
       " 226,\n",
       " 62,\n",
       " 267,\n",
       " 35,\n",
       " 596,\n",
       " 14,\n",
       " 183,\n",
       " 28,\n",
       " 29,\n",
       " 129,\n",
       " 204,\n",
       " 622,\n",
       " 257,\n",
       " 269,\n",
       " 294,\n",
       " 583,\n",
       " 580,\n",
       " 500,\n",
       " 507,\n",
       " 503,\n",
       " 69,\n",
       " 72,\n",
       " 41,\n",
       " 108,\n",
       " 110,\n",
       " 109,\n",
       " 129,\n",
       " 660,\n",
       " 626,\n",
       " 99,\n",
       " 1,\n",
       " 21,\n",
       " 145,\n",
       " 39,\n",
       " 78,\n",
       " 661,\n",
       " 428,\n",
       " 517,\n",
       " 512,\n",
       " 525,\n",
       " 174,\n",
       " 610,\n",
       " 662,\n",
       " 226,\n",
       " 12,\n",
       " 127,\n",
       " 114,\n",
       " 78,\n",
       " 7,\n",
       " 586,\n",
       " 202,\n",
       " 232,\n",
       " 30,\n",
       " 103]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bod = t.bodies_tokenized_0.iloc[0]\n",
    "b=1\n",
    "idx=1\n",
    "bod = t[\"bodies_tokenized_\" + str(b)].iloc[idx]\n",
    "while len(bod) < 60:\n",
    "    bod.append(0)\n",
    "bod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "motivated-prince",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "train_stop = int(len(t)* 0.9)\n",
    "train_df = t.iloc[0:train_stop]\n",
    "test_df = t.iloc[train_stop +1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "creative-confidentiality",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_body(arr_body, max_len_body):\n",
    "    if isinstance(arr_body, list):\n",
    "        if len(arr_body) > max_len_body:\n",
    "            arr_body[:max_len_body]\n",
    "        else:\n",
    "            while len(arr_body) < max_len_body:\n",
    "                arr_body.append(0)\n",
    "    else:\n",
    "        arr_body = []\n",
    "        while len(arr_body) < max_len_body:\n",
    "            arr_body.append(0)\n",
    "    return arr_body\n",
    "\n",
    "def create_batch(df, max_len_body):\n",
    "    y = []\n",
    "    arr_news = []\n",
    "    arr_author = []\n",
    "    arr_bodies = np.zeros((len(df), 3, max_len_body))\n",
    "    #arr_bodies_1 = np.zeros((len(df), max_len_body))\n",
    "    #arr_bodies_2 = np.zeros((len(df), max_len_body))\n",
    "    #arr_bodies_3 = np.zeros((len(df), max_len_body))\n",
    "    arr_bodies_1 = np.zeros((len(df), max_len_body))\n",
    "    \n",
    "    arr_bodies_2 = np.zeros((len(df), max_len_body))\n",
    "    \n",
    "    arr_bodies_3 = np.zeros((len(df), max_len_body))\n",
    "\n",
    "    for idx in range(len(df)):\n",
    "        news_click = df.filter(regex=\"news_id\")\n",
    "        news_click = news_click.iloc[idx,:]\n",
    "        news_click = np.array(news_click, dtype=float)\n",
    "        \n",
    "        author_id = df.filter(regex = \"author_num\")\n",
    "        author_id = author_id.iloc[idx,:]\n",
    "        author_id = np.array(author_id)\n",
    "        \n",
    "        #for b in range(3):\n",
    "            #bod = df[\"bodies_tokenized_\" + str(b)].iloc[idx]\n",
    "            #bod = pad_body(bod, max_len_body)\n",
    "            #for elem in range(max_len_body):\n",
    "              #arr_bodies[idx][b][elem] = bod[elem]\n",
    "        bod_1 = df[\"bodies_tokenized_0\"].iloc[idx]\n",
    "        bod_1 = pad_body(bod_1, max_len_body)\n",
    "        for elem in range(max_len_body):\n",
    "            arr_bodies_1[idx][elem] = bod_1[elem]\n",
    "        \n",
    "        bod_2 = df[\"bodies_tokenized_2\"].iloc[idx]\n",
    "        bod_2 = pad_body(bod_2, max_len_body)\n",
    "        for elem in range(max_len_body):\n",
    "            arr_bodies_2[idx][elem] = bod_2[elem]\n",
    "        \n",
    "        bod_3 = df[\"bodies_tokenized_3\"].iloc[idx]\n",
    "        bod_3 = pad_body(bod_3, max_len_body)\n",
    "        for elem in range(max_len_body):\n",
    "            arr_bodies_3[idx][elem] = bod_3[elem]\n",
    "        \n",
    "        #arr_bodies_2 = bod[\"bodies_tokenized_1\"].values\n",
    "        #arr_bodies_3 = bod[\"bodies_tokenized_2\"].values\n",
    "        #print(arr_bodies_1)\n",
    "            \n",
    "        \n",
    "        #context = df.iloc[idx, 1:25].values\n",
    "        \n",
    "        #hist_count = df.hist_count.iloc[idx]\n",
    "        \n",
    "        #temp = (video_watches, search_tokens,context, hist_count)\n",
    "        \n",
    "        arr_news.append(news_click)\n",
    "        arr_author.append(author_id)\n",
    "        y.append(df.label.iloc[idx])\n",
    "   \n",
    "    return arr_news, arr_author, y, arr_bodies_1, arr_bodies_2, arr_bodies_3\n",
    "\n",
    "arr_news, arr_author, y, arr_bodies_1, arr_bodies_2,arr_bodies_3  = create_batch(train_df, max_len_body=60) \n",
    "arr_news_test, arr_author_test, y_test, arr_bodies_1_test, arr_bodies_2_test, arr_bodies_3_test = create_batch(test_df,max_len_body=60 )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "cutting-treasury",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6630"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bodies.shape = (6630, 3, 60)\n",
    "# arr_news.shape = (6630,8)\n",
    "# arr_author.shape = (6630, 8)\n",
    "len(arr_bodies_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-perth",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "baking-wheel",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(arr_news) == len(y) == len(arr_author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "short-malaysia",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stupid-cookbook",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "abstract-fraud",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27045"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "secondary-powell",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_news_words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "coupled-progress",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommenderV1(num_of_articles, \n",
    "                  article_emb_dim, \n",
    "                  vocab_size,\n",
    "                  vocab_size_body,\n",
    "                  hidden_size1, \n",
    "                  hidden_size2, \n",
    "                  hidden_size3):\n",
    "    \n",
    "    # bodies.shape = (6630, 3, 60)\n",
    "    # arr_news.shape = (6630,8)\n",
    "    # arr_author.shape = (6630, 8)\n",
    "    \n",
    "    input_article = layers.Input(shape=(8,), name=\"articles\")\n",
    "    article_emb = layers.Embedding(input_dim=num_of_articles, output_dim=article_emb_dim, input_length=8)(input_article)\n",
    "    article = layers.GlobalAveragePooling1D()(article_emb)\n",
    "    #watches = tf.math.l2_normalize(watches_emb, axis=-1)\n",
    "    #watches = tf.reduce_mean(watches, axis=1)\n",
    "    \n",
    "    input_author = layers.Input(shape=(8,), name=\"authors\")\n",
    "    author_emb = layers.Embedding(input_dim=vocab_size, output_dim=16, input_length=8)(input_author)\n",
    "    author = layers.GlobalAveragePooling1D()(author_emb)\n",
    "    \n",
    "    combined_author_article = layers.Concatenate()([author, article]) # Note, embeddings are concatinated\n",
    "    \n",
    "    x = layers.Dense(1024, activation=\"relu\")(combined_author_article)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(512, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(num_of_articles)(x)\n",
    "    \n",
    "    # body\n",
    "    inputs_body_1 = layers.Input(shape=(60,), name=\"body_1\")\n",
    "    inputs_body_2 = layers.Input(shape=(60,), name=\"body_2\")\n",
    "    inputs_body_3 = layers.Input(shape=(60,), name=\"body_3\")\n",
    "\n",
    "    embedding = layers.Embedding(input_dim=vocab_size_body, output_dim=16) # input_length is the length of the sequence of words in a sentence. is typically used when having a sequence of symbols as input (think sequence of words).\n",
    "    body_1 = embedding(inputs_body_1)\n",
    "    body_2 = embedding(inputs_body_2)\n",
    "    body_3 = embedding(inputs_body_3)\n",
    "    bodies = layers.Concatenate()([body_1, body_2, body_3])\n",
    "    conv = layers.Conv1D(128, 5, activation=\"relu\")(bodies)\n",
    "    pool = layers.GlobalMaxPooling1D()(conv) # puts it into a lower dimension\n",
    "    \n",
    "    # flatten volume, then FC -> RELU etc\n",
    "    x_body = layers.Flatten()(pool)\n",
    "    x_body = layers.Dense(1024, activation=\"relu\")(x_body) # new\n",
    "    x_body = layers.Dropout(0.2)(x_body)\n",
    "    x_body = layers.Dense(512, activation=\"relu\")(x_body) # new\n",
    "    x_body = layers.Dropout(0.2)(x_body)\n",
    "    \n",
    "    #apply FC to match the number of nodes\n",
    "    x_body = layers.Dense(num_of_articles)(x_body)\n",
    "    \n",
    "    x = layers.Concatenate()([x, x_body])\n",
    "    \n",
    "    \n",
    "    \n",
    "    #inputs_body = [layers.Input(shape=(60,), name=\"body_\"+str(i)) for i in range(3)]\n",
    "    #body_emb = layers.Embedding(input_dim=vocab_size_body, output_dim=300)\n",
    "    #bodies = [body_emb(inp) for inp in inputs_body]\n",
    "    #bodies_concatinated = layers.Concatenate()(bodies)\n",
    "    #print(\"Bodies concatinated\",bodies_concatinated.shape)\n",
    "    #print(\"Article after pool\",article.shape)\n",
    "    #body = layers.GlobalAveragePooling1D()(bodies_concatinated)\n",
    "    #print(\"Bodies pool\", body.shape)\n",
    "        #searches = tf.math.l2_normalize(search_emb, axis=-1)\n",
    "        #searches = tf.reduce_mean(searches, axis=1)\n",
    "        #x = layers.Concatenate()([watches, searches])\n",
    "    #x = layers.Concatenate()([x, body])\n",
    "    \n",
    "    #video_watches, search_tokens,context, hist_count\n",
    "    #x = layers.Dense(hidden_size1, activation=\"relu\")(x)\n",
    "    #x = layers.Dense(hidden_size2, activation=\"relu\")(x)\n",
    "    #x = layers.Dense(hidden_size3, activation=\"relu\")(x)\n",
    "    x = layers.Dense(1024, activation=\"relu\")(x) # new\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(512, activation=\"relu\")(x) # new\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "\n",
    "    output = layers.Dense(num_of_articles, activation=\"softmax\")(x)\n",
    "    \n",
    "    #model = keras.Model([input_article, input_author], output)\n",
    "\n",
    "    model = keras.Model([input_article, input_author, inputs_body_1, inputs_body_2, inputs_body_3], output)\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", \n",
    "                  optimizer=tf.keras.optimizers.Adam(0.03),\n",
    "                 metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "retired-hostel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_55\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "body_1 (InputLayer)             [(None, 60)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "body_2 (InputLayer)             [(None, 60)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "body_3 (InputLayer)             [(None, 60)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_183 (Embedding)       (None, 60, 16)       432720      body_1[0][0]                     \n",
      "                                                                 body_2[0][0]                     \n",
      "                                                                 body_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "authors (InputLayer)            [(None, 8)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "articles (InputLayer)           [(None, 8)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_118 (Concatenate)   (None, 60, 48)       0           embedding_183[0][0]              \n",
      "                                                                 embedding_183[1][0]              \n",
      "                                                                 embedding_183[2][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_182 (Embedding)       (None, 8, 16)        2592        authors[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_181 (Embedding)       (None, 8, 16)        7840        articles[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 56, 128)      30848       concatenate_118[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_153 (G (None, 16)           0           embedding_182[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_152 (G (None, 16)           0           embedding_181[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_16 (Global (None, 128)          0           conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_117 (Concatenate)   (None, 32)           0           global_average_pooling1d_153[0][0\n",
      "                                                                 global_average_pooling1d_152[0][0\n",
      "__________________________________________________________________________________________________\n",
      "flatten_16 (Flatten)            (None, 128)          0           global_max_pooling1d_16[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_229 (Dense)               (None, 1024)         33792       concatenate_117[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_232 (Dense)               (None, 1024)         132096      flatten_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 1024)         0           dense_229[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1024)         0           dense_232[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_230 (Dense)               (None, 512)          524800      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_233 (Dense)               (None, 512)          524800      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512)          0           dense_230[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 512)          0           dense_233[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_231 (Dense)               (None, 490)          251370      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_234 (Dense)               (None, 490)          251370      dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_119 (Concatenate)   (None, 980)          0           dense_231[0][0]                  \n",
      "                                                                 dense_234[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_235 (Dense)               (None, 1024)         1004544     concatenate_119[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 1024)         0           dense_235[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_236 (Dense)               (None, 512)          524800      dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 512)          0           dense_236[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_237 (Dense)               (None, 490)          251370      dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,972,942\n",
      "Trainable params: 3,972,942\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_of_articles = num_articles    # 1 is for padding\n",
    "vocab_size = len(author_to_id) +1\n",
    "\n",
    "#author_to_id, id_to_author\n",
    "# tuneable hyper-parameters\n",
    "article_emb_dim = 16   \n",
    "hidden_size1 = 256 # TODO: experiment with hidden sizes: num_of_articles * constant\n",
    "hidden_size2 = 64\n",
    "hidden_size3 = 32\n",
    "model = recommenderV1(num_of_articles, \n",
    "                      article_emb_dim, \n",
    "                      vocab_size,\n",
    "                      vocab_size_body,\n",
    "                      hidden_size1,\n",
    "                      hidden_size2,\n",
    "                      hidden_size3)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "mature-toyota",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "104/104 [==============================] - 4s 32ms/step - loss: 111.1721 - accuracy: 0.0700\n",
      "Epoch 2/10\n",
      "104/104 [==============================] - 3s 31ms/step - loss: 4.0236 - accuracy: 0.0980\n",
      "Epoch 3/10\n",
      "104/104 [==============================] - 3s 32ms/step - loss: 27.0118 - accuracy: 0.1183\n",
      "Epoch 4/10\n",
      "104/104 [==============================] - 3s 32ms/step - loss: 3.5631 - accuracy: 0.1169\n",
      "Epoch 5/10\n",
      "104/104 [==============================] - 3s 32ms/step - loss: 3.5598 - accuracy: 0.1261\n",
      "Epoch 6/10\n",
      "104/104 [==============================] - 3s 32ms/step - loss: 3.5434 - accuracy: 0.1242\n",
      "Epoch 7/10\n",
      "104/104 [==============================] - 3s 32ms/step - loss: 3.5591 - accuracy: 0.1229\n",
      "Epoch 8/10\n",
      "104/104 [==============================] - 3s 32ms/step - loss: 3.5432 - accuracy: 0.1202\n",
      "Epoch 9/10\n",
      "104/104 [==============================] - 3s 32ms/step - loss: 3.5443 - accuracy: 0.1170\n",
      "Epoch 10/10\n",
      "104/104 [==============================] - 3s 33ms/step - loss: 3.5708 - accuracy: 0.1213\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit([np.array(arr_news), np.array(arr_author), arr_bodies_1, arr_bodies_2, arr_bodies_3], \n",
    "                 np.array(y), \n",
    "                 epochs=10, \n",
    "                 batch_size=64,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "standing-meditation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#arr_news_test, arr_author_test\n",
    "preds = model.predict([np.array(arr_news_test), np.array(arr_author_test), arr_bodies_1_test, arr_bodies_2_test, arr_bodies_3_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "promotional-samba",
   "metadata": {},
   "outputs": [],
   "source": [
    "recs = tf.nn.top_k(preds, k=10, sorted=True, name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "floppy-quantum",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(recs[1]) == len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-reviewer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "explicit-hometown",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06046195652173962"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def precision_k(predictions, y_true):\n",
    "    hit = 0\n",
    "    precision = 0\n",
    "    k = 10\n",
    "    for i in range(len(predictions)):\n",
    "        for j in range(k): # k=10\n",
    "            if predictions[i][j] == y_true[i]:\n",
    "                hit += 1\n",
    "        precision += hit / k\n",
    "        hit = 0\n",
    "    return precision / len(predictions)\n",
    "p = precision_k(recs[1], y_test)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "powered-fault",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def plot_history(history):\n",
    "    # Source: https://realpython.com/python-keras-text-classification/\n",
    "    acc = history.history['accuracy']\n",
    "    #val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    #val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    #plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    #plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training loss')\n",
    "    plt.legend()\n",
    "\n",
    "plot_history(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "obvious-comment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnum_of_articles = num_articles \\noutput_dim = num_of_articles\\n\\nmlp = nlp_model(num_of_articles, vocab_size_body, output_dim)\\nmlp.summary()\\n'"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def nlp_model(num_classes, \n",
    "              vocab_size, \n",
    "             output_dim):\n",
    "    inputs_body = [layers.Input(shape=(60,), name=\"body_\"+str(i)) for i in range(3)]\n",
    "    embedding = layers.Embedding(input_dim=vocab_size, output_dim=300) # input_length is the length of the sequence of words in a sentence. is typically used when having a sequence of symbols as input (think sequence of words).\n",
    "    bodies = [embedding(bod) for bod in inputs_body]\n",
    "    bodies = layers.Concatenate()(bodies)\n",
    "    conv = layers.Conv1D(128, 5, activation=\"relu\")(bodies)\n",
    "    pool = layers.GlobalMaxPooling1D()(conv) # puts it into a lower dimension\n",
    "    \n",
    "    # flatten volume, then FC -> RELU etc\n",
    "    x = layers.Flatten()(pool)\n",
    "    x = layers.Dense(num_classes*10, activation=\"relu\")(x) # new\n",
    "    x = layers.Dense(num_classes*5, activation=\"relu\")(x) # new\n",
    "    \n",
    "    #apply FC to match the number of nodes\n",
    "    x = layers.Dense(num_classes)(x)\n",
    "    #output = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    \n",
    "    model = keras.Model(inputs_body, x)\n",
    "    #model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(0.03),metrics=[\"accuracy\"])\n",
    "\n",
    "    return model\n",
    "\"\"\"\n",
    "num_of_articles = num_articles \n",
    "output_dim = num_of_articles\n",
    "\n",
    "mlp = nlp_model(num_of_articles, vocab_size_body, output_dim)\n",
    "mlp.summary()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "supreme-teach",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnum_of_articles = num_articles    \\nvocab_size = len(author_to_id) +1\\narticle_emb_dim = 16   \\nhidden_size1 = 512 # TODO: experiment with hidden sizes: num_of_articles * constant\\nhidden_size2 = 64\\nhidden_size3 = 32\\noutput_size = 300\\nmlp = mlp_model(num_of_articles, \\n                      article_emb_dim, \\n                      vocab_size,\\n                      vocab_size_body,\\n                      hidden_size1,\\n                      hidden_size2,\\n                      hidden_size3,\\n                       output_size)\\nmlp.summary()\\n'"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mlp_model(num_of_articles, \n",
    "                  article_emb_dim, \n",
    "                  vocab_size,\n",
    "                  vocab_size_body,\n",
    "                  hidden_size1, \n",
    "                  hidden_size2, \n",
    "                  hidden_size3,\n",
    "                 output_size):\n",
    "    \n",
    "    # bodies.shape = (6630, 3, 60)\n",
    "    # arr_news.shape = (6630,8)\n",
    "    # arr_author.shape = (6630, 8)\n",
    "    \n",
    "    input_article = layers.Input(shape=(8,), name=\"articles\")\n",
    "    article_emb = layers.Embedding(input_dim=num_of_articles, output_dim=article_emb_dim, input_length=8)(input_article)\n",
    "    article = layers.GlobalAveragePooling1D()(article_emb)\n",
    "    \n",
    "    input_author = layers.Input(shape=(8,), name=\"authors\")\n",
    "    author_emb = layers.Embedding(input_dim=vocab_size, output_dim=16, input_length=8)(input_author)\n",
    "    author = layers.GlobalAveragePooling1D()(author_emb)\n",
    "    \n",
    "    x = layers.Concatenate()([author, article]) # Note, embeddings are concatinated\n",
    "    \n",
    "    #video_watches, search_tokens,context, hist_count\n",
    "    x = layers.Dense(num_of_articles*2, activation=\"relu\")(x)\n",
    "    #x = layers.Dense(hidden_size2, activation=\"relu\")(x)\n",
    "    #x = layers.Dense(hidden_size3, activation=\"relu\")(x)\n",
    "    \n",
    "    # FC layer to match size\n",
    "    x = layers.Dense(num_of_articles, activation=\"relu\")(x)\n",
    "\n",
    "    model = keras.Model([input_article, input_author], x)\n",
    "    #model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(0.03),metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model\n",
    "\"\"\"\n",
    "num_of_articles = num_articles    \n",
    "vocab_size = len(author_to_id) +1\n",
    "article_emb_dim = 16   \n",
    "hidden_size1 = 512 # TODO: experiment with hidden sizes: num_of_articles * constant\n",
    "hidden_size2 = 64\n",
    "hidden_size3 = 32\n",
    "output_size = 300\n",
    "mlp = mlp_model(num_of_articles, \n",
    "                      article_emb_dim, \n",
    "                      vocab_size,\n",
    "                      vocab_size_body,\n",
    "                      hidden_size1,\n",
    "                      hidden_size2,\n",
    "                      hidden_size3,\n",
    "                       output_size)\n",
    "mlp.summary()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "tropical-measure",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hist_mlp = mlp.fit([np.array(arr_news), np.array(arr_author)], np.array(y), epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "under-fancy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_47\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "body_0 (InputLayer)             [(None, 60)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "body_1 (InputLayer)             [(None, 60)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "body_2 (InputLayer)             [(None, 60)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_151 (Embedding)       (None, 60, 300)      90000       body_0[0][0]                     \n",
      "                                                                 body_1[0][0]                     \n",
      "                                                                 body_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_88 (Concatenate)    (None, 60, 900)      0           embedding_151[0][0]              \n",
      "                                                                 embedding_151[1][0]              \n",
      "                                                                 embedding_151[2][0]              \n",
      "__________________________________________________________________________________________________\n",
      "authors (InputLayer)            [(None, 8)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "articles (InputLayer)           [(None, 8)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 56, 128)      576128      concatenate_88[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "embedding_150 (Embedding)       (None, 8, 16)        2592        authors[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_149 (Embedding)       (None, 8, 16)        7840        articles[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 128)          0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_131 (G (None, 16)           0           embedding_150[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_130 (G (None, 16)           0           embedding_149[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 128)          0           global_max_pooling1d_6[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_87 (Concatenate)    (None, 32)           0           global_average_pooling1d_131[0][0\n",
      "                                                                 global_average_pooling1d_130[0][0\n",
      "__________________________________________________________________________________________________\n",
      "dense_164 (Dense)               (None, 4900)         632100      flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_162 (Dense)               (None, 980)          32340       concatenate_87[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_165 (Dense)               (None, 2450)         12007450    dense_164[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_163 (Dense)               (None, 490)          480690      dense_162[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_166 (Dense)               (None, 490)          1200990     dense_165[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_89 (Concatenate)    (None, 980)          0           dense_163[0][0]                  \n",
      "                                                                 dense_166[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_167 (Dense)               (None, 490)          480690      concatenate_89[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_168 (Dense)               (None, 490)          240590      dense_167[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 15,751,410\n",
      "Trainable params: 15,751,410\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def main_model(num_of_articles, article_emb_dim, vocab_size,\n",
    "               hidden_size1,\n",
    "               hidden_size2,\n",
    "               hidden_size3,\n",
    "               output_size,\n",
    "               vocab_size_body,\n",
    "               output_dim):\n",
    "    mlp = mlp_model(num_of_articles, \n",
    "                          article_emb_dim, \n",
    "                          vocab_size,\n",
    "                          vocab_size_body,\n",
    "                          hidden_size1,\n",
    "                          hidden_size2,\n",
    "                          hidden_size3,\n",
    "                          output_size)\n",
    "    nlp = nlp_model(num_of_articles, vocab_size_body, output_dim)\n",
    "    \n",
    "    combined = layers.Concatenate()([mlp.output, nlp.output])\n",
    "    \n",
    "    x = layers.Dense(num_of_articles, activation=\"relu\")(combined)\n",
    "    x = layers.Dense(num_of_articles, activation=\"softmax\")(x)\n",
    "    model = keras.Model([mlp.input, nlp.input], x)\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", \n",
    "                  optimizer=tf.keras.optimizers.Adam(0.03),\n",
    "                  metrics=[\"accuracy\"]\n",
    "                 )\n",
    "    return model\n",
    "\n",
    "    \n",
    "    \n",
    "num_of_articles = num_articles    # 1 is for padding\n",
    "vocab_size = len(author_to_id) +1\n",
    "\n",
    "#author_to_id, id_to_author\n",
    "# tuneable hyper-parameters\n",
    "article_emb_dim = 16   \n",
    "hidden_size1 = 256 # TODO: experiment with hidden sizes: num_of_articles * constant\n",
    "hidden_size2 = 64\n",
    "hidden_size3 = 32\n",
    "\n",
    "test_model = main_model(num_of_articles,\n",
    "                        article_emb_dim,\n",
    "                        vocab_size,\n",
    "                        vocab_size_body,\n",
    "                          hidden_size1,\n",
    "                          hidden_size2,\n",
    "                          hidden_size3,\n",
    "                          output_size, output_dim)\n",
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "focused-groove",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 60) for input KerasTensor(type_spec=TensorSpec(shape=(None, 60), dtype=tf.float32, name='body_0'), name='body_0', description=\"created by layer 'body_0'\"), but it was called on an input with incompatible shape (None, 3, 60).\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "in user code:\n\n    /Users/eivindfalun/opt/anaconda3/envs/dnnrs/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    /Users/eivindfalun/opt/anaconda3/envs/dnnrs/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /Users/eivindfalun/opt/anaconda3/envs/dnnrs/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /Users/eivindfalun/opt/anaconda3/envs/dnnrs/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /Users/eivindfalun/opt/anaconda3/envs/dnnrs/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /Users/eivindfalun/opt/anaconda3/envs/dnnrs/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\n        outputs = model.train_step(data)\n    /Users/eivindfalun/opt/anaconda3/envs/dnnrs/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:754 train_step\n        y_pred = self(x, training=True)\n    /Users/eivindfalun/opt/anaconda3/envs/dnnrs/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:1012 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    /Users/eivindfalun/opt/anaconda3/envs/dnnrs/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py:425 call\n        inputs, training=training, mask=mask)\n    /Users/eivindfalun/opt/anaconda3/envs/dnnrs/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py:569 _run_internal_graph\n        assert x_id in tensor_dict, 'Could not compute output ' + str(x)\n\n    AssertionError: Could not compute output KerasTensor(type_spec=TensorSpec(shape=(None, 490), dtype=tf.float32, name=None), name='dense_168/Softmax:0', description=\"created by layer 'dense_168'\")\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-331-39f9a5e0ea68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                  \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                  \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                  \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m                 )\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dnnrs/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dnnrs/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dnnrs/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    869\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dnnrs/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    725\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 726\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dnnrs/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2970\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dnnrs/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dnnrs/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dnnrs/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dnnrs/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dnnrs/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: in user code:\n\n    /Users/eivindfalun/opt/anaconda3/envs/dnnrs/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    /Users/eivindfalun/opt/anaconda3/envs/dnnrs/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /Users/eivindfalun/opt/anaconda3/envs/dnnrs/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /Users/eivindfalun/opt/anaconda3/envs/dnnrs/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /Users/eivindfalun/opt/anaconda3/envs/dnnrs/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /Users/eivindfalun/opt/anaconda3/envs/dnnrs/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\n        outputs = model.train_step(data)\n    /Users/eivindfalun/opt/anaconda3/envs/dnnrs/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:754 train_step\n        y_pred = self(x, training=True)\n    /Users/eivindfalun/opt/anaconda3/envs/dnnrs/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:1012 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    /Users/eivindfalun/opt/anaconda3/envs/dnnrs/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py:425 call\n        inputs, training=training, mask=mask)\n    /Users/eivindfalun/opt/anaconda3/envs/dnnrs/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py:569 _run_internal_graph\n        assert x_id in tensor_dict, 'Could not compute output ' + str(x)\n\n    AssertionError: Could not compute output KerasTensor(type_spec=TensorSpec(shape=(None, 490), dtype=tf.float32, name=None), name='dense_168/Softmax:0', description=\"created by layer 'dense_168'\")\n"
     ]
    }
   ],
   "source": [
    "hist = test_model.fit([np.array(arr_news), np.array(arr_author), bodies], \n",
    "                 np.array(y), \n",
    "                 epochs=10, \n",
    "                 batch_size=64,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-ribbon",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
